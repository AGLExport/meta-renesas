From 0bb3d9c4e7306ba1d8a149bf288cbc0aab0d9210 Mon Sep 17 00:00:00 2001
From: Shinobu Uehara <shinobu.uehara.xc@renesas.com>
Date: Fri, 14 Jun 2013 15:47:20 +0900
Subject: [PATCH 521/715] mmc: tmio: Add return processing from PIO fall back
 to DMA mode

DMA channel request processing is added if it is released.

Signed-off-by: Shinobu Uehara <shinobu.uehara.xc@renesas.com>
(cherry picked from commit 782ff32fbe6d366bd0e76713fbd4145d4b4181a1)

Signed-off-by: Ryo Kataoka <ryo.kataoka.wt@renesas.com>
---
 drivers/mmc/host/tmio_mmc_dma.c |   40 +++++++++++++++++++++++++++++++++++++++
 1 file changed, 40 insertions(+)

diff --git a/drivers/mmc/host/tmio_mmc_dma.c b/drivers/mmc/host/tmio_mmc_dma.c
index b49d79c..7f7ef93 100644
--- a/drivers/mmc/host/tmio_mmc_dma.c
+++ b/drivers/mmc/host/tmio_mmc_dma.c
@@ -79,6 +79,7 @@ static void tmio_mmc_start_dma_rx(struct tmio_mmc_host *host)
 	}
 
 	tmio_mmc_disable_mmc_irqs(host, TMIO_STAT_RXRDY);
+	tmio_mmc_enable_dma(host, true);
 
 	/* The only sg element can be unaligned, use our bounce buffer then */
 	if (!aligned) {
@@ -157,6 +158,7 @@ static void tmio_mmc_start_dma_tx(struct tmio_mmc_host *host)
 	}
 
 	tmio_mmc_disable_mmc_irqs(host, TMIO_STAT_TXRQ);
+	tmio_mmc_enable_dma(host, true);
 
 	/* The only sg element can be unaligned, use our bounce buffer then */
 	if (!aligned) {
@@ -206,9 +208,47 @@ pio:
 		desc, cookie);
 }
 
+static bool tmio_mmc_filter(struct dma_chan *chan, void *arg);
+
 void tmio_mmc_start_dma(struct tmio_mmc_host *host,
 			       struct mmc_data *data)
 {
+	struct tmio_mmc_data *pdata = host->pdata;
+	dma_cap_mask_t mask;
+	struct dma_chan *chan;
+
+	if (pdata->dma && (!host->chan_rx || !host->chan_tx)) {
+		if (host->chan_rx) {
+			chan = host->chan_rx;
+			host->chan_rx = NULL;
+			dma_release_channel(chan);
+		}
+		if (host->chan_tx) {
+			chan = host->chan_tx;
+			host->chan_tx = NULL;
+			dma_release_channel(chan);
+		}
+		dma_cap_zero(mask);
+		dma_cap_set(DMA_SLAVE, mask);
+
+		if (pdata->dma_filter) {
+			host->chan_rx = dma_request_channel(mask,
+						pdata->dma_filter,
+						pdata->dma->chan_priv_rx);
+
+			host->chan_tx = dma_request_channel(mask,
+						pdata->dma_filter,
+						pdata->dma->chan_priv_tx);
+		} else {
+			host->chan_rx = dma_request_channel(mask,
+						tmio_mmc_filter,
+						pdata->dma->chan_priv_rx);
+
+			host->chan_tx = dma_request_channel(mask,
+						tmio_mmc_filter,
+						pdata->dma->chan_priv_tx);
+		}
+	}
 	if (data->flags & MMC_DATA_READ) {
 		if (host->chan_rx)
 			tmio_mmc_start_dma_rx(host);
-- 
1.7.10.4

